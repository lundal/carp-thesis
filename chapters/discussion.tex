\TODO
How it was to implement a CA system inside an FPGA.
Distributed and local communication allows it to fills entire FPGA.
Perfect for 2D structure, extra routing challenges with 3D.

%==============================================================================%

\section{Performance}

As with the previous 3D design, the performance of certain modules scale with the matrix width (X), as they operate on one row of cells at a time.
The main ones are are development, configuration and readback.

In addition, synthesis parameters allow the performance of some components to be scaled up or down in trade for resource usage.
[Rules Tested In Parallel] can be increased to greatly improve the performance of development.
The speed is equivalent to the previous design when set to 2, but designs have been successfully implemented with values of up to 16, depending on matrix size and configuration.
[LUT Configuration Bits] controls the speed of CA configuration, which at maximum value is currently a bit slower than the previous design.
Lower values ease routing however, which allow implementation of designs with larger matrices.

The speed of the DFT can be adjusted by setting the number of DSP slices and the transform size.
However, as the fitness module is exchangeable, parameters have to be specified at the top of the respective VHDL file.

\subsection{Communication}

\todo{intro?}

The latency is measured by averaging the times for 100000 pings.
Each ping is the execution a read\_type instruction followed by retrieval of the type.
Since all retrieval functions check the amount of buffer data first, this totals to 2.5 PCI Express round trips\footnotemark plus one one instruction execution per ping.

\footnotetext{
    Sending to the board requires no confirmation, and can thus be seen as half of a round trip.
}

The throughput is measured by 1000 executions of the read\_types instruction followed by retrieval of their data.
The calculated number of words transferred are then compared to the time.
The reads are interleaved so that an empty buffer is never encountered.

\begin{table}[!ht]
    \renewcommand{\arraystretch}{1.4}
    \centering
    \begin{tabular}{c|c|c}
        \bfseries Mode & \bfseries Latency & \bfseries Throughput \\
        \hline
        Normal & \SI{60.3}{\micro\second} & 2.1 MB/s \\
        Low-latency & \SI{7.3}{\micro\second} & 2.1 MB/s \\
    \end{tabular}
    \caption[Communication performance]{
        Performance of the PCI Express communication unit.
    }
    \label{tab:communication-performance}
\end{table}

The results are presented in \tablename~\ref{tab:communication-performance}.
The latency appears to be decent in low-latency mode, which is almost equivalent to the first buffer check succeeding.
Normal mode is around \SI{50}{\micro\second} slower as it fails the first check and therefore waits.
The delay between checks is only set to \SI{1}{\micro\second}, but it is likely increased due to operating system scheduling.

The throughput is not exceptional at around 1\% of the 256MB/s that the PCI Express endpoint block is theoretically capable of \cite{ug672}.
This is likely due to the simplistic PIO scheme that require that all transfers go through the CPU.
However, this is not as bad as one would first assume.
Following is a short analysis of the desired throughput for the example program in \figurename~\ref{fig:example-program}.

\begin{figure}[!ht]
\begin{lstlisting}[xleftmargin=0.35\textwidth]
initialize()
while counter[0] != 128:
    develop()
    config()
    step(128)
    send_fitness()
    swap_cell_storage()
    counter[0]++
\end{lstlisting}
\caption[Example program] {
    Example program.
}
\label{fig:example-program}
\end{figure}

Assume the following synthesis parameters:
[LUT Configuration Bits] maximized to 8,
[Rules Tested In Parallel] set to 8,
[Rule Amount] set to 256,
[Fitness] set to DFT with transform size of 128 and 16-bit output values,
and the matrix sized to 10x10x8.

That brings development speed to 3.2 cycles/cell and configuration speed to 1.6 cycles/cell (plus overhead)\footnotemark.
The DFT work in parallel with the rest of the design, therefore adding no further delay, and produces 32 words of data after each 128 steps.
With 800 cells, the time for each loop iteration then becomes a little over $3.2 \cdot 800 + 1.6 \cdot 800 + 128 = 3968$ cycles.
32 words per 3968 cycles at 125 Mhz constitutes around 4 MB/s.
The communication unit can therefore supply 50\% of the desired throughput, which should be acceptable for normal operation.
A faster communication module is of course desirable, but it requires much more advanced logic that communicates through DMA with a custom driver.

\footnotetext{
    The execution time of each instruction is detailed in Appendix~\ref{app:isa}.
}

\subsection{Cellular Automaton}

Previous hardware designs have been profiled by the test program in \figurename~\ref{fig:test-program}, with an 8x8 matrix and 6 development rules.
It is mainly a stress-test of the CA stepping speed, and the fastest speed was 6.3 seconds in both \cite{djupdal2003sblock} and \cite{stovneng2014sblock}.

\begin{figure}[!ht]
\begin{lstlisting}[xleftmargin=0.34\textwidth]
initialize()
while counter[0] != 10000:
    config()
    step(50000)
    readback()
    swap_cell_storage()
    read_types()
    read_states()
    develop()
    counter[0]++
\end{lstlisting}
\caption[Test program] {
    Updated test program from \cite{djupdal2003sblock}.
}
\label{fig:test-program}
\end{figure}

On the new platform, the program completes in 8.2 seconds.
This is exactly twice of what it should take given 125 MHz speed.
Further study show that the PCI Express Endpoint Block is to blame as it actually halves the frequency to 62.5 MHz at the user side.
This fact is barely mentioned in \cite{ug672} and is never referred to by the IP core generator or example design.
It has also gone unnoticed in simulations since the communication module has been replaced with a special simulation version.

During synthesis and implementation, the constraint of 125 MHz have been applied to all signals though, and the critical paths require about 7.9 ns.
This means that is possible to (again) separate the communication module into a slower clock domain to return the remaining design to full speed.
The new design would then be 35\% faster instead of 30\% slower for this test.

Unfortunately, the late discovery of the problem leaves insufficient time to fix it, but it should be fairly straightforward.
For the purpose of other speed and resource comparisons, the platform is therefore assumed to run at 125 MHz.

%==============================================================================%

\section{Resource Usage}

With a complete rewrite, it is interesting to see the differences in resource usage between equivalent setups.
Due to architectural differences, the performance of the new design can not be perfectly matched with that of the previous, but it should be close enough to determine the general trend.
The following configuration assume that the design is running at the intended 125 MHz.

The most closely matching configuration is:
[LUT Configuration Bits] maximized to 2 in 2D and 8 in 3D,
[Rules Tested In Parallel] set to 2,
[Rule Amount] set to 256,
[Type Bits] set to 5,
[State Bits] set to 1,
[Fitness] set to Live Count
and buffer sizes set to 256.
Other parameters do not significantly influence resource usage, functionality or performance.

In 3D, the live counter is four times faster, configuration is half as fast, and readback is an eight as fast.
In 2D, configuation is a quarter as fast for 32x32 matrices.
Due to different scaling in the old 2D design, no other matrix sizes have equivalent performance and have therefore been left out of the comparison.
The results are presented in \tablename~\ref{tab:resource-usage}.

\begin{table}[!ht]
    \renewcommand{\arraystretch}{1.4}
    \centering
    \begin{tabular}{c|c|c|c|c|c|c|c|c}
        \bfseries Matrix & \multicolumn{2}{c|}{\bfseries SRL16} & \multicolumn{2}{c|}{\bfseries LUTs} & \multicolumn{2}{c|}{\bfseries Registers} & \multicolumn{2}{c}{\bfseries BRAMs} \\
        \bfseries (XxYxZ) & \bfseries Total & \bfseries \% & \bfseries Old & \bfseries New & \bfseries Old & \bfseries New & \bfseries Old & \bfseries New \\
        \hline
        32x32 & 2048 & 32.0 & 14858 & 11277 & 16259 & 7043 & 38 & 53 \\
        8x8x4 & 2048 & 32.0 & 6529 & 6265 & 6011 & 4495 & 55 & 47 \\
        8x8x8 & 4096 & 63.9 & 7668 & 8374 & 5726 & 4913 & 50 & 47 \\
        8x16x4 & 4096 & 63.9 & 8234 & 8252 & 6531 & 4957 & 50 & 47 \\
        10x10x8 & 6400 & 99.9 & – & 11313 & – & 5832 & – & 52 \\
    \end{tabular}
    \caption[Resource usage]{
        Resource usage without DFT compared to the old design with most equivalently configured setup and performance.
        The old numbers are from \cite{stovneng2014sblock}.
    }
    \label{tab:resource-usage}
\end{table}

The new design appears to be slightly more efficient in 3D.
It uses about the same amount of LUTs, but substantially fewer registers and slightly fewer BRAMs.
This is a bit surprising considering the four times larger adder tree.
In 2D, both LUT and register usage are drastically reduced.

The size of the matrix is limited by the number of available 16-bit shift registers (SRL16s), as each sblock uses 2 in 2D and 8 in 3D.
Since the new design is more finely tunable, larger matrices can be fitted onto the chip.
A 10x10x8 matrix design uses 99.9\% of the 6408 shift registers on the Spartan-6 LX45T, and has been successfully implemented and tested with the above configuration.
It will even implement with [Rules Tested In Parallel] increased to 6 and [Fitness] set to a DFT using 32 out of the 58 available DSP slices.

At this point, routing becomes the main problem, as there are still many logic resources left but nearly all paths are of critical length.
Both the shift registers and DSP slices are spread across the entire FPGA.
This means that values from the sblocks must be routed from the entire FPGA into one location when counting and then spread out to the entire FPGA again when computing the DFT.

It 2D, the design clogs up before all shift registers can be used.
The largest matrix that has therefore been successfully implemented is 50x50.
This is with the above configuration, except for [LUT Configuration Bits] and [Rules Tested In Parallel] reduced to 1, but with [Fitness] set to DFT.

%==============================================================================%

\section{Challenges}
\label{sec:challenges}

\TODO

ISE lacks support for VHDL-2008, which is required to use custom types with generics.
All array signals must therefore be converted to and from std\_logic\_vectors before exiting and after entering modules.
Hopefully, this is implemented in a way that is both organized and understandable.

\TODO

%==============================================================================%

\section{Future work}

The most important issue that needs remediation is the downclocking caused by the PCI Express Endpoint, which halves the performance of the entire design.
The relatively straightforward fix is to reintroduce a second clock domain.
Clock domain traversals might be tricky however.

The second most important issue, and by far the most challenging, is the relatively low throughput of the communication module.
Improvement requires the use of DMA in combination with a custom device driver.
It has the added benefit of allowing programs to run without superuser rights, although the driver will need them instead.

It is possible to further parameterize some parts of the design.
First is the number of rows that can be read from Cell Storage, which will allow for higher CA configuration and readback speeds.
Another is the number of cycles used by the Live Counter, which will allow the user to trade speed for less resource usage and easier routing.
Finally, the Live Counter could be made exchangeable by an interface similar to fitness, to allow more complex evaluation of the output.
It might even be baked into fitness.

The current design only executes one instruction at a time to keep everything simple and organized.
However, there are many circumstances where the next instruction do not impact the currently executing one and can be safely run in parallel to gain a small speedup.
It should be possible to implement by letting a form of hazard detection module handle the interlocking signals for the main pipeline, such that the run signal is asserted as soon as all modules that the next instruction depends on are done.

Since some instructions have the ability to create deadlocks, some sort of watchdog-timer might be useful to allow recovery of the system.
The only alternative is cycling the power, but that will break the PCI Express connection which necessitates a reboot of the host as well.

%==============================================================================%

\section{Warnings}

During synthesis, the design produces a substantial amount of warnings, although significantly less than the previous.
Most arise from unused signals due to the selected synthesis parameters and are completely benign.
In addition, a good deal stem from the provided PCI Express Endpoint Core, which can also safely be ignored.

In an effort to reduce clutter and direct focus at more important information, most of the benign warnings are blocked by a filter.
The remaining could not be removed without possibly causing trouble later, due to a limited filter system.
For a standard 3D design, this reduces the amount of warnings from around 500 to about 50 \footnotemark.

\footnotetext{
    Synthesizing from within ISE produces around 730 warnings, opposed to the 500 when using XST directly.
    However, both are filtered down to about 50.
}
